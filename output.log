nohup: ignoring input
[2024-03-10 15:33:44,299] torch.distributed.run: [WARNING] 
[2024-03-10 15:33:44,299] torch.distributed.run: [WARNING] *****************************************
[2024-03-10 15:33:44,299] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-10 15:33:44,299] torch.distributed.run: [WARNING] *****************************************
| distributed init (rank 0): env://
Namespace(amp=True, batch_size=1, device='cuda', dist_backend='nccl', dist_url='env://', distributed=True, epochs=100, gpu=0, img_type='rgb', label_type='Treelabel', lambda1=0.4, lambda2=0.8, lr=0.0001, momentum=0.4, name='', num_classes=6, output_dir='./u2net_for_atten_preprocess/multi_train/OSP/', print_freq=20, rank=0, resume='./Pixel_MLP/multi_train/OSP/model_030', start_epoch=0, sync_bn=False, test_only=False, train_data_path='/data2/chaoyi/HSI_Dataset/V2/train/', val_data_path='/data2/chaoyi/HSI_Dataset/V2/test/', weight_decay=0.0001, workers=8, world_size=2)
Start Tensorboard with "tensorboard --logdir=runs", view at http://localhost:6006/
Creating data loaders
Creating model
| distributed init (rank 1): env://
Start training
Epoch: [0]  [  0/304]  eta: 0:07:09  lr: 0.000100  loss: 13.406719  acc: 0.315308  time: 1.4143  data: 0.4070  max mem: 6309
Epoch: [0]  [ 20/304]  eta: 0:01:45  lr: 0.000100  loss: 10.353425  acc: 0.467404  time: 0.3195  data: 0.0046  max mem: 6319
Epoch: [0]  [ 40/304]  eta: 0:01:30  lr: 0.000100  loss: 7.159052  acc: 0.535735  time: 0.3155  data: 0.0040  max mem: 6319
Epoch: [0]  [ 60/304]  eta: 0:01:21  lr: 0.000100  loss: 7.514178  acc: 0.425198  time: 0.3146  data: 0.0045  max mem: 6319
Epoch: [0]  [ 80/304]  eta: 0:01:13  lr: 0.000100  loss: 9.359595  acc: 0.328124  time: 0.3138  data: 0.0048  max mem: 6319
Epoch: [0]  [100/304]  eta: 0:01:06  lr: 0.000100  loss: 6.945377  acc: 0.579843  time: 0.3156  data: 0.0051  max mem: 6319
Epoch: [0]  [120/304]  eta: 0:00:59  lr: 0.000100  loss: 9.514540  acc: 0.404490  time: 0.3164  data: 0.0055  max mem: 6319
