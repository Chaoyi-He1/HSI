nohup: ignoring input
[2024-03-02 18:06:19,137] torch.distributed.run: [WARNING] 
[2024-03-02 18:06:19,137] torch.distributed.run: [WARNING] *****************************************
[2024-03-02 18:06:19,137] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-02 18:06:19,137] torch.distributed.run: [WARNING] *****************************************
| distributed init (rank 0): env://
Namespace(amp=True, batch_size=1, device='cuda', dist_backend='nccl', dist_url='env://', distributed=True, epochs=100, gpu=0, img_type='ALL', label_type='Treelabel', lambda1=0.4, lambda2=0.8, lr=0.0001, momentum=0.4, name='', num_classes=6, output_dir='./u2net_for_atten_preprocess/multi_train/OSP/', print_freq=20, rank=0, resume='./Pixel_MLP/multi_train/OSP/model_030', start_epoch=0, sync_bn=False, test_only=False, train_data_path='/data2/chaoyi/HSI_Dataset/V2/train/', val_data_path='/data2/chaoyi/HSI_Dataset/V2/test/', weight_decay=0.0001, workers=8, world_size=2)
Start Tensorboard with "tensorboard --logdir=runs", view at http://localhost:6006/
Creating data loaders
| distributed init (rank 1): env://
Creating model
Start training
Epoch: [0]  [  0/304]  eta: 1:14:00  lr: 0.000100  loss: 13.458366  acc: 0.205709  time: 14.6080  data: 12.5868  max mem: 6600
Epoch: [0]  [ 20/304]  eta: 0:14:17  lr: 0.000100  loss: 10.299608  acc: 0.613032  time: 2.4410  data: 1.6211  max mem: 6610
Epoch: [0]  [ 40/304]  eta: 0:10:56  lr: 0.000100  loss: 6.781187  acc: 0.482007  time: 1.9233  data: 0.2807  max mem: 6610
Epoch: [0]  [ 60/304]  eta: 0:09:54  lr: 0.000100  loss: 7.836503  acc: 0.590007  time: 2.3399  data: 0.3207  max mem: 6610
Epoch: [0]  [ 80/304]  eta: 0:09:40  lr: 0.000100  loss: 9.412674  acc: 0.506775  time: 3.0600  data: 0.0683  max mem: 6610
Epoch: [0]  [100/304]  eta: 0:08:31  lr: 0.000100  loss: 6.856860  acc: 0.529304  time: 2.1665  data: 0.2301  max mem: 6610
Epoch: [0]  [120/304]  eta: 0:07:43  lr: 0.000100  loss: 9.202247  acc: 0.370116  time: 2.5673  data: 0.4934  max mem: 6610
Epoch: [0]  [140/304]  eta: 0:06:45  lr: 0.000100  loss: 7.324666  acc: 0.566271  time: 2.1853  data: 0.8269  max mem: 6610
Epoch: [0]  [160/304]  eta: 0:06:06  lr: 0.000100  loss: 6.214624  acc: 0.652956  time: 3.0957  data: 0.0461  max mem: 6610
