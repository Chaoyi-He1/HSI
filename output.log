nohup: ignoring input
[2024-03-02 18:06:19,137] torch.distributed.run: [WARNING] 
[2024-03-02 18:06:19,137] torch.distributed.run: [WARNING] *****************************************
[2024-03-02 18:06:19,137] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-02 18:06:19,137] torch.distributed.run: [WARNING] *****************************************
| distributed init (rank 0): env://
Namespace(amp=True, batch_size=1, device='cuda', dist_backend='nccl', dist_url='env://', distributed=True, epochs=100, gpu=0, img_type='ALL', label_type='Treelabel', lambda1=0.4, lambda2=0.8, lr=0.0001, momentum=0.4, name='', num_classes=6, output_dir='./u2net_for_atten_preprocess/multi_train/OSP/', print_freq=20, rank=0, resume='./Pixel_MLP/multi_train/OSP/model_030', start_epoch=0, sync_bn=False, test_only=False, train_data_path='/data2/chaoyi/HSI_Dataset/V2/train/', val_data_path='/data2/chaoyi/HSI_Dataset/V2/test/', weight_decay=0.0001, workers=8, world_size=2)
Start Tensorboard with "tensorboard --logdir=runs", view at http://localhost:6006/
Creating data loaders
| distributed init (rank 1): env://
Creating model
Start training
Epoch: [0]  [  0/304]  eta: 1:14:00  lr: 0.000100  loss: 13.458366  acc: 0.205709  time: 14.6080  data: 12.5868  max mem: 6600
